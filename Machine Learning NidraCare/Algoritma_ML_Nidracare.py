# -*- coding: utf-8 -*-
"""(FIX) Algoritma ML NidraCare

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qo2THUs8Mjc-rYonP0k-mRRUJylelIpO

# Install Library
"""

!pip install kagglehub
!pip install tensorflowjs

"""# Import Package"""

import kagglehub
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler
import joblib

"""# Data Load

Pada tahap ini, kami mengambil dataset dari sumber berikut :

https://www.kaggle.com/datasets/uom190346a/sleep-health-and-lifestyle-dataset
"""

# Download dataset
path = kagglehub.dataset_download("uom190346a/sleep-health-and-lifestyle-dataset")
print("Path to dataset files:", path)

# Load dataset
df = pd.read_csv('/kaggle/input/sleep-health-and-lifestyle-dataset/Sleep_health_and_lifestyle_dataset.csv')

print("Initial data preview:")
df.head()

"""# Data Understanding

### Cek Informasi Data

Dalam Cell dibawah ini dapat kita ketahui bahwa dataset terdiri dari 374 barus data dengan 13 kolom
"""

df.info()

"""### Pengecekan Duplikasi Data

Pada tahapan ini kita dapat melakukan pengecekan duplikasi data dengan df.`duplicated().sum(). `Setelah melakukan pengecekan ternyata tidak terdapat indikasi duplikasi data.
"""

print('\nJumlah Data Duplikat : ', df.duplicated().sum())

"""### Pengecekan Missing Value

pada tahapan ini kita dapat melakukan pengecekan missing value dalam dataset tersebut diantaranya menggunakan fungsi .`isnull().sum()` untuk mengetahui missing value di setiap kolom
"""

print("\nMissing values:")
print(df.isnull().sum())

"""Berdasarkan output diatas ditemukan adanya missing value pada kolom rating sebanyak 219 baris sehingga pada bagian Sleep Disorder nantinya akan dilakukan Penanganan Missing Value.

### Distribusi Data
"""

gender_counts = df['Gender'].value_counts()
gender_percentages = gender_counts / len(df) * 100


plt.figure(figsize=(8, 8))
plt.pie(gender_percentages, labels=gender_percentages.index, autopct='%1.1f%%', startangle=90, colors=['skyblue', 'pink'])
plt.title('Gender Distribution')
plt.axis('equal')
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(df['Age'], bins=20, kde=True)
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(12, 6))
sns.histplot(df['Sleep Duration'], bins=10, kde=True)
plt.title('Distribution of Sleep Duration')
plt.xlabel('Sleep Duration (hours)')
plt.ylabel('Frequency')
plt.show()

"""### Pengelompokan Tipe Data

pada tahapan ini saya mengecek tipe data terhadap kolom yang akan di gunakan untuk permodelan, disini saya menggunakan `df.select_dtypes(include=['number']).columns.tolist()` dan `df.select_dtypes(include=['object']).columns.tolist()`. dapat dilihat pada output dibawah kode tersebut apa saja kolom dengan tipe data numerik dam kolom dengan data kategorikal
"""

numerical_cols = df.select_dtypes(include=['number']).columns.tolist()
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

print("\nNumerical columns:")
print(numerical_cols)
print("\nCategorical columns:")
print(categorical_cols)

numeric_data = df.select_dtypes(include=['float64', 'int64'])
print("\nDescriptive statistics for numerical variables:")
print(numeric_data.describe())

"""# Data Preprocessing

### Penanganan Missing Value

Dalam tahapan ini, kami tidak menggunakan kolom 'Person ID', 'Blood Pressure', 'Occupation', dan 'Heart Rate' karena dianggap cukup rumit untuk diintegrasikan dan dimasukkan ke dalam formulir, mengingat proses pengambilan parametriknya yang cukup kompleks
"""

df.drop(columns=['Person ID','Blood Pressure','Occupation', 'Heart Rate'], axis=1, inplace=True)

"""Pada tahapan ini kami mengisi nilai NaN atau missing value pada kolom sleep disorder dengan data 'Normal' untuk penanganan missing value"""

df['Sleep Disorder'] = np.where(df['Sleep Disorder'].isna(), 'Normal', df['Sleep Disorder'])
df.head()

print("\nUpdated data sample:")
print(df.head())

"""### Label Encoding

Pada tahapan ini kami melakukan encoding untuk data kategorikal menjadi data numerik agar bisa digunakan dalam permodelan lebih mudah.
"""

from sklearn.preprocessing import LabelEncoder
gender_encoder = LabelEncoder()
bmi_encoder = LabelEncoder()


df['Gender'] = gender_encoder.fit_transform(df['Gender'])
df['BMI Category'] = bmi_encoder.fit_transform(df['BMI Category'])

df['Sleep Disorder'] = df['Sleep Disorder'].map({'Normal': 0, 'Sleep Apnea': 1, 'Insomnia': 2})

print(df.head())

"""### Split & Train Data

Sebelum memasuki tahap pemodelan, saya membagi dataset menjadi dua bagian utama, yaitu fitur (x) dan label/target (y). Fitur adalah semua kolom kecuali kolom 'Sleep Disorder', sedangkan label adalah kolom 'Sleep Disorder' itu sendiri. Setelah itu, saya melakukan pembagian data menjadi:

* Data latih (training set) sebanyak 80%

* Data uji (testing set) sebanyak 20%

Pembagian ini dilakukan menggunakan fungsi train_test_split dari Scikit-Learn dengan parameter random_state=42 agar hasil pembagian data bersifat reproducible (konsisten setiap kali dijalankan).
"""

# Split Data Training
x = df.drop('Sleep Disorder', axis=1)
y = df['Sleep Disorder']

print("x : ", x.shape)
print("y :", y.shape)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

"""Langkah selanjutnya pada tahapan ini saya melakukan normalisasi menggunakan `StandardScaler()` dengan tujuan meningkatkan hasil prediksi lebih akurat dan stabil dan menyimpannya ke scaler.save untuk kebutuhan back end"""

# Normalisasi
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert target to categorical format for Keras
y_train_cat = to_categorical(y_train, num_classes=3)
y_test_cat = to_categorical(y_test, num_classes=3)

# Simpan scaler ke file
joblib.dump(scaler, 'scaler.save')

"""# Modelling Classification

Disini kami melakukan implementasi metode klasifikasi menggunakan model Artficial Neural Network berbasis Keras dan TensorFlow. Tujuannya adalah untuk memprediksi kategori gangguan tidur berdasarkan data.

Pertama, kami melakukan penetapan random seed untuk memastikan bahwa proses pelatihan model menghasilkan hasil yang konsisten dan dapat direproduksi saat kode dijalankan ulang.
"""

# Set random seed
import random
random.seed(42)
tf.random.set_seed(42)
np.random.seed(42)

"""Selanjutnya, kami membuat model klasisifikasinya dengan dua arsitektur, yaitu :


*   Simple Neural Network
*   Deep Neural Network : Setiap model terdiri dari beberapa layer Dense dengan aktivasi ReLU, disertai dengan Batch Normalization dan Dropout untuk mengurangi risiko overfitting.


"""

def create_model(name, input_shape):
    if name == 'simple_nn':
        model = keras.Sequential([
            layers.Dense(64, activation='relu', input_shape=(input_shape,)),
            layers.BatchNormalization(),
            layers.Dropout(0.3),
            layers.Dense(32, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.2),
            layers.Dense(3, activation='softmax')
        ])
    elif name == 'deep_nn':
        model = keras.Sequential([
            layers.Dense(128, activation='relu', input_shape=(input_shape,)),
            layers.BatchNormalization(),
            layers.Dropout(0.4),
            layers.Dense(64, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.3),
            layers.Dense(32, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.2),
            layers.Dense(3, activation='softmax')
        ])
    else:  # Default model
        model = keras.Sequential([
            layers.Dense(32, activation='relu', input_shape=(input_shape,)),
            layers.Dense(16, activation='relu'),
            layers.Dense(3, activation='softmax')
        ])

    # Compile model
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )


    return model

"""Setelah model arsitekturnya dibuat, kami melakukan pelatihan dan evaluasi model. Pada tahap ini, model dilatih menggunakan data training dengan validasi silang sebesar 20%. Selama proses pelatihan, digunakan *Early Stopping* untuk menghentikan pelatohan jika tidak terjadi perbaikan pada nilai *validation loss* . Model kemudian dievaluasi menggunakan data uji untuk memperoleh metrik akurasi, presisi, recall, dan F1-score."""

# Define models
models = {
    'Simple Neural Network': create_model('simple_nn', X_train_scaled.shape[1]),
    'Deep Neural Network': create_model('deep_nn', X_train_scaled.shape[1])
}

# Store results
results = []

# Training and evaluation
for name, model in models.items():
    print(f"\nTraining {name}...")

    early_stopping = keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True
    )

    print(model.summary())

    history = model.fit(
        X_train_scaled, y_train_cat,
        epochs=100,
        batch_size=16,
        validation_split=0.2,
        callbacks=[early_stopping],
        verbose=1
    )

    # Evaluate model
    test_loss, test_acc = model.evaluate(X_test_scaled, y_test_cat, verbose=0)

    # Get predictions
    y_pred_prob = model.predict(X_test_scaled)
    y_pred = np.argmax(y_pred_prob, axis=1)

    # Calculate metrics
    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)

    # Store results
    results.append({
        'Model': name,
        'Accuracy': test_acc,
        'Precision': report['weighted avg']['precision'],
        'Recall': report['weighted avg']['recall'],
        'F1-Score': report['weighted avg']['f1-score']
    })

"""Tahapan berikutnya adalah melakukan perbandingan hasil evaluasi kinerja dari kedua model, dubandingkan dalam bentuk tabel untuk menentukan model dengan performa terbaik, dan hasil yang didapatkan adalah Simple Neural Network

# Evaluation
"""

# buat perbandingan dataframe
evaluation_df = pd.DataFrame(results)
print("\nModel Performance Comparison:")
print(evaluation_df)

"""Selanjutnya adalah melakukan visualisasi hasilnya dalam bentuk grafik learning curve (akurasi dan loss), confusion matrix, serta analisis feature importance menggunakan model sederhana dengan regularisasi L1."""

# history plot
for name, model in models.items():
    plt.figure(figsize=(12, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title(f'{name} - Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='lower right')

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title(f'{name} - Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper right')
    plt.tight_layout()
    plt.show()

    # Confusion matrix
    plt.figure(figsize=(8, 6))
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Normal', 'Sleep Apnea', 'Insomnia'],
                yticklabels=['Normal', 'Sleep Apnea', 'Insomnia'])
    plt.title(f'Confusion Matrix - {name}')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.show()

"""## Feature Importance Analysis"""

# ----------------------------------
# Feature Importance Analysis using a simpler model
# ----------------------------------
# We'll use a simple model with L1 regularization to estimate feature importance

print("\nAnalyzing feature importance...")
feature_importance_model = keras.Sequential([
    layers.Dense(16, activation='relu', input_shape=(X_train_scaled.shape[1],),
                kernel_regularizer=keras.regularizers.l1(0.01)),
    layers.Dense(3, activation='softmax')
])

feature_importance_model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

feature_importance_model.fit(
    X_train_scaled, y_train_cat,
    epochs=50,
    batch_size=16,
    verbose=0
)

# Get the weights from the first layer
weights = feature_importance_model.layers[0].get_weights()[0]
importance = np.sum(np.abs(weights), axis=1)

# Create feature importance DataFrame
feature_names = x.columns
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})
importance_df = importance_df.sort_values('Importance', ascending=False)

# Plot feature importance
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importance Analysis')
plt.tight_layout()
plt.show()

print("\nFeature Importance Ranking:")
print(importance_df)

"""Kemudian model disimpan dalam format HDF5 untuk digunakan kembali dalam proses prediksi."""

best_model_name = evaluation_df.loc[evaluation_df['Accuracy'].idxmax(), 'Model']
best_model = models[best_model_name]

print(f"\nSaving the best model: {best_model_name}")
best_model.save('sleep_disorder_prediction_model.h5')
print("Model saved as 'sleep_disorder_prediction_model.h5'")

"""# Testing"""

print("\nExample prediction with the best model:")
# Create a sample input
sample = X_test.iloc[0:1]
sample_scaled = scaler.transform(sample)

# Make prediction
prediction_prob = best_model.predict(sample_scaled)
prediction_class = np.argmax(prediction_prob, axis=1)[0]

# Map prediction to class name
disorder_map = {0: 'Normal', 1: 'Sleep Apnea', 2: 'Insomnia'}
predicted_disorder = disorder_map[prediction_class]

print("Sample input features:", sample.values)
print("Prediction probabilities:", prediction_prob[0])
print(f"Predicted sleep disorder: {predicted_disorder} (class {prediction_class})")
print(f"Actual sleep disorder class: {y_test.iloc[0]} ({disorder_map[y_test.iloc[0]]})")

"""Kode berikut adalah fungsi prediksi berbasis input manual dari pengguna, dimana data yang dimasukkan akan diproses sesuai dengan skema preprocessing training sebelum diprediksi oleh model."""

def predict_sleep_disorder(model, scaler):
    print("\n--- Input Data for Prediction ---")

    # Get user input
    gender = input("Enter Gender (Male/Female): ").strip().capitalize()
    age = int(input("Enter Age: "))
    sleep_duration = float(input("Enter Sleep Duration (hours): "))
    physical_activity_level = int(input("Enter Physical Activity Level (minutes per day): "))
    stress_level = int(input("Enter Stress Level (1-10): "))
    bmi_category = input("Enter BMI Category (Normal, Normal Weight, Obese, Overweight): ").strip().capitalize()
    daily_steps = int(input("Enter Daily Steps: "))
    quality_of_sleep = int(input("Enter Quality of Sleep (1-10): "))

    # Mapping sesuai yang kamu pakai waktu training
    gender_map = {'Male': 1, 'Female': 2}
    bmi_map = {'Normal': 0, 'Normal weight': 1, 'Obese': 2, 'Overweight': 3}

    # Validasi input gender
    if gender not in gender_map:
        print(f"Invalid Gender: {gender}. Must be Male, Female")
        return

    if bmi_category not in bmi_map:
        print(f"Invalid BMI Category: {bmi_category}. Must be Normal, Normal Weight, Obese, or Overweight.")
        return

    # Convert to numeric
    gender_encoded = gender_map[gender]
    bmi_encoded = bmi_map[bmi_category]

    # Create a DataFrame sesuai kolom training
    user_data = pd.DataFrame({
        'Gender': [gender_encoded],
        'Age': [age],
        'Sleep Duration': [sleep_duration],
        'Physical Activity Level': [physical_activity_level],
        'Stress Level': [stress_level],
        'BMI Category': [bmi_encoded],
        'Daily Steps': [daily_steps],
        'Quality of Sleep': [quality_of_sleep]
    })

    # Urutkan kolom sesuai urutan training data
    user_data = user_data[x.columns]

    # Scaling
    user_data_scaled = scaler.transform(user_data)

    # Predict
    prediction_prob = model.predict(user_data_scaled)
    prediction_class = np.argmax(prediction_prob, axis=1)[0]

    # Map prediction hasil ke label
    disorder_map = {0: 'Normal', 1: 'Sleep Apnea', 2: 'Insomnia'}
    predicted_disorder = disorder_map[prediction_class]

    # Output
    print("\n--- Prediction Results ---")
    print("Input Data:", user_data.values[0])
    print("Prediction Probabilities:", prediction_prob[0])
    print(f"Predicted Sleep Disorder: {predicted_disorder}")

# Contoh panggil
predict_sleep_disorder(best_model, scaler)

"""# Export Model"""

import tensorflowjs as tfjs
#TFJS
tfjs_target_dir = 'tfjs_model/'
tfjs.converters.save_keras_model(model, tfjs_target_dir)
print(f"> TFJS model tersimpan di folder: {tfjs_target_dir}/")

import shutil
shutil.make_archive('tfjs_model', 'zip', 'tfjs_model')

import tensorflow as tf
print(tf.__version__)